{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOC9VkZwksS4Z9PkRYqO3A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Katerynaa91/Machine-Learning/blob/main/models_for_github_todownload_and_replace.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLae1DnjFoX7"
      },
      "outputs": [],
      "source": [
        "#Initializing parameters\n",
        "# A function to initialize parameters\n",
        "\n",
        "def initialize_weights_and_bias(dimension):\n",
        "    # dimension - number of input features\n",
        "    w = np.full((dimension,1),0.01)\n",
        "    b = 0.0\n",
        "    return w, b\n",
        "\n",
        "# Forward Propagation\n",
        "def sigmoid(z):\n",
        "    y_head = 1/(1+np.exp(-z))\n",
        "    return y_head\n",
        "def forward_propagation(w,b,x_train,y_train):\n",
        "    z = np.dot(w.T,x_train) + b\n",
        "    y_head = sigmoid(z) # probabilistic 0-1\n",
        "    loss = -1*y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head) #log-loss function\n",
        "    cost = (np.sum(loss))/x_train.shape[1]      # x_train.shape[1]  is for scaling; loss function total sum for all samples\n",
        "    return cost\n",
        "\n",
        "\n",
        "# Gradient Descent\n",
        "# In backward propagation we will use y_head that found in forward progation\n",
        "# Therefore instead of writing backward propagation method, lets combine forward propagation and backward propagation\n",
        "\n",
        "def forward_backward_propagation(w,b,x_train,y_train, eps=1e-5):\n",
        "\n",
        "    # forward propagation\n",
        "\n",
        "    z = np.dot(w.T,x_train) + b\n",
        "    y_head = sigmoid(z)\n",
        "    loss = -1*y_train*np.log(y_head+eps)-(1-y_train)*np.log(1-y_head+eps)\n",
        "    cost = (np.sum(loss))/x_train.shape[1]      # x_train.shape[1]  is for scaling\n",
        "\n",
        "    # backward propagation\n",
        "\n",
        "    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))/x_train.shape[1] # x_train.shape[1]  is for scaling\n",
        "    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]                 # x_train.shape[1]  is for scaling\n",
        "    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias}\n",
        "\n",
        "    return cost,gradients\n",
        "\n",
        "# Updating(learning) parameters\n",
        "\n",
        "def update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n",
        "    cost_list = []\n",
        "    index = []\n",
        "\n",
        "    # updating(learning) parameters is number_of_iterarion times\n",
        "\n",
        "    for i in range(number_of_iterarion):\n",
        "\n",
        "        # make forward and backward propagation and find cost and gradients\n",
        "\n",
        "        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n",
        "        cost_list.append(cost)\n",
        "\n",
        "        # lets update\n",
        "\n",
        "        w = w - learning_rate * gradients[\"derivative_weight\"]\n",
        "        b = b - learning_rate * gradients[\"derivative_bias\"]\n",
        "        index.append(i)\n",
        "\n",
        "    # we update(learn) parameters weights and bias\n",
        "\n",
        "    parameters = {\"weight\": w,\"bias\": b}\n",
        "    plt.plot(index,cost_list)\n",
        "    plt.xticks(index,rotation='vertical')\n",
        "    plt.xlabel(\"Number of Iterarion\")\n",
        "    plt.ylabel(\"Cost\")\n",
        "    plt.show()\n",
        "\n",
        "    return parameters, gradients, cost_list\n",
        "def predict(w,b,x_test):\n",
        "\n",
        "    # x_test is a input for forward propagation\n",
        "\n",
        "    z = sigmoid(np.dot(w.T,x_test)+b)\n",
        "    Y_prediction = np.zeros((1,x_test.shape[1]))\n",
        "\n",
        "    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n",
        "    # if z is smaller than 0.5, our prediction is sign zero (y_head=0)\n",
        "\n",
        "    for i in range(z.shape[1]):\n",
        "        if z[0,i]<= 0.5:\n",
        "            Y_prediction[0,i] = 0\n",
        "        else:\n",
        "            Y_prediction[0,i] = 1\n",
        "\n",
        "    return Y_prediction\n",
        "\n",
        "# Lets put them all together\n",
        "\n",
        "def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n",
        "\n",
        "    # initialize\n",
        "\n",
        "    dimension =  x_train.shape[0]  # that is 4096\n",
        "    w,b = initialize_weights_and_bias(dimension)\n",
        "\n",
        "    # do not change learning rate\n",
        "    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n",
        "\n",
        "    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n",
        "    y_prediction_train = predict(parameters[\"weight\"],parameters[\"bias\"],x_train)\n",
        "\n",
        "    # Print train/test Errors\n",
        "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n",
        "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n",
        "\n",
        "\n",
        "logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 0.00001, num_iterations = 50)"
      ]
    }
  ]
}
